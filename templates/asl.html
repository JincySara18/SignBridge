<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>SignBridge - ASL</title>
  <style>
    body { background-color: #011a17; color: white; font-family: sans-serif; }
    #video-wrapper { margin: 20px auto; max-width: 420px; aspect-ratio: 4/3; border: 4px solid #1cc9aa; border-radius: 20px; overflow: hidden; }
    video, canvas { width: 100%; height: 100%; object-fit: cover; }
    #prediction { background: #83d8cd; color: black; text-align: center; padding: 10px; margin: 10px auto; border-radius: 10px; max-width: 420px; }
    #logBox { background: #fff; color: #000; padding: 10px; font-size: 13px; border-radius: 8px; max-width: 420px; margin: 16px auto; white-space: pre-wrap; max-height: 160px; overflow-y: auto; }
    .speaker { display: flex; justify-content: center; margin: 20px; }
    button { margin: 12px auto; display: block; padding: 8px 16px; font-size: 16px; border-radius: 6px; border: none; background: #83d8cd; color: black; }
  </style>
</head>
<body>

  <button onclick="toggleCamera()">üîÅ Toggle Camera (Fullscreen)</button>

  <div id="video-wrapper">
    <video id="webcam" autoplay playsinline muted></video>
    <canvas id="overlay" width="640" height="480"></canvas>
  </div>

  <div id="prediction">No hand detected</div>
  <div class="speaker">
    <button onclick="speakPrediction()">üîä Speak</button>
  </div>
  <pre id="logBox">[LOGS]\n</pre>

  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/hands/hands.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js"></script>

  <script>
    const videoElement = document.getElementById("webcam");
    const canvasElement = document.getElementById("overlay");
    const canvasCtx = canvasElement.getContext("2d");
    const logBox = document.getElementById("logBox");
    const predictionBox = document.getElementById("prediction");

    let useBackCamera = true;
    let currentStream = null;
    let camera = null;
    let currentPrediction = "No hand detected";

    function log(msg) {
      logBox.textContent += msg + "\n";
      console.log(msg);
    }

    const hands = new Hands({
      locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/hands/${file}`
    });

    hands.setOptions({
      maxNumHands: 1,
      modelComplexity: 1,
      minDetectionConfidence: 0.7,
      minTrackingConfidence: 0.7
    });

    hands.onResults((results) => {
      canvasCtx.save();
      canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);

      if (results.multiHandLandmarks && results.multiHandLandmarks.length > 0) {
        const rawLandmarks = results.multiHandLandmarks[0];
        drawConnectors(canvasCtx, rawLandmarks, HAND_CONNECTIONS, { color: "#00FF00", lineWidth: 2 });
        drawLandmarks(canvasCtx, rawLandmarks, { color: "#FF0000", radius: 3 });
        predictSign(rawLandmarks);
      } else {
        predictionBox.textContent = "No hand detected";
      }

      canvasCtx.restore();
    });

    async function predictSign(landmarks) {
      const base = landmarks[0];
      const normalized = landmarks.flatMap(lm => [lm.x - base.x, lm.y - base.y, lm.z - base.z]);
      const res = await fetch("/predict", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({ landmarks: normalized }),
      });

      const data = await res.json();
      currentPrediction = data.prediction || "No hand detected";
      predictionBox.textContent = currentPrediction;
    }

    async function startCamera() {
      if (currentStream) currentStream.getTracks().forEach(track => track.stop());

      let constraints = {
        video: {
          facingMode: { exact: useBackCamera ? "environment" : "user" }
        }
      };

      try {
        currentStream = await navigator.mediaDevices.getUserMedia(constraints);
        videoElement.srcObject = currentStream;
        log(`üì∑ Started ${useBackCamera ? "back" : "front"} camera.`);

        videoElement.onloadedmetadata = () => {
          videoElement.play();
          canvasElement.width = videoElement.videoWidth;
          canvasElement.height = videoElement.videoHeight;

          if (camera) camera.stop();
          camera = new Camera(videoElement, {
            onFrame: async () => await hands.send({ image: videoElement }),
            width: videoElement.videoWidth,
            height: videoElement.videoHeight
          });
          camera.start();
        };
      } catch (err) {
        log(`‚ùå Camera error: ${err.name}`);
        if (useBackCamera && err.name === "OverconstrainedError") {
          log("‚ö†Ô∏è Back camera not available. Switching to front.");
          useBackCamera = false;
          startCamera();
        }
      }
    }

    async function toggleCamera() {
      useBackCamera = !useBackCamera;

      // Request fullscreen
      try {
        if (document.documentElement.requestFullscreen) {
          await document.documentElement.requestFullscreen();
          log("üñ•Ô∏è Fullscreen mode requested.");
        }
      } catch (err) {
        log("‚ö†Ô∏è Fullscreen error: " + err.message);
      }

      // Try switching camera
      startCamera();
    }

    function speakPrediction() {
      if (!currentPrediction || currentPrediction === "No hand detected") return;
      const utterance = new SpeechSynthesisUtterance(currentPrediction);
      utterance.lang = "en-US";
      speechSynthesis.cancel();
      speechSynthesis.speak(utterance);
    }

    window.addEventListener("DOMContentLoaded", () => {
      startCamera();
    });
  </script>
</body>
</html>
